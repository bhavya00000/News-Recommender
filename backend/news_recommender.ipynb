{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrVTUv6u5efC"
      },
      "source": [
        "# **DATA FETCHING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UF_O8ic35Uh8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_news(api_key):\n",
        "    url = f'https://newsapi.org/v2/top-headlines?country=us&apiKey={api_key}'\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    articles = []\n",
        "    for article in data['articles']:\n",
        "        articles.append({'title': article['title'], 'content': article['description']})\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Insert your API key here\n",
        "api_key = '2bb8b3db5aa248228b5b197119330502'\n",
        "articles = fetch_news(api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWF3eRU25lU1",
        "outputId": "efc5a097-19ac-4602-96dc-29aaa5051190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'title': 'Family of Corey Comperatore, Trump rallygoer shot dead, struggles with loss - BBC.com', 'content': 'Corey Comperatore was killed in the Trump rally assassination attempt. His wife is furious at the security failures that led to his death.'}, {'title': 'EU executive to adopt tariffs on Chinese EVs after vote - Reuters', 'content': \"The European Union will press ahead with hefty tariffs on China-made electric vehicles, the EU executive said on Friday, even after the bloc's largest economy Germany rejected them, exposing a rift over its biggest trade row with Beijing in a decade.\"}, {'title': 'ChatGPT’s ‘canvas’ interface makes it easier to write and code - The Verge', 'content': 'OpenAI has launched a new “canvas” interface for ChatGPT that allows users to adjust sections of text or code generated by the chatbot without a full rewrite.'}, {'title': 'Rocket Report: Falcon 9 second stage stumbles; Japanese rocket nears the end - Ars Technica', 'content': '“I’m pretty darn confident I’m going to have a good day on Friday.”…'}, {'title': '[Removed]', 'content': '[Removed]'}, {'title': 'Port strike update: Dockworkers going back to work as talks continue amid tentative agreement - WPVI-TV', 'content': 'Strikers are celebrating a tentative agreement that raises their pay 62% over the next six years.'}, {'title': 'Will the Jobs Report Stoke Stock-Market Fireworks? - The Wall Street Journal', 'content': 'The U.S. economy added 254,000 jobs in September, a sign the labor market is robustly healthy. The unemployment rate fell to 4.1%. Follow along for live updates on stocks, bonds and markets, including the Dow Jones Industrial Average, S&P 500 and Nasdaq Compo…'}, {'title': 'Obama to rally support for Harris across key battleground states - CNN', 'content': 'Former President Barack Obama plans to commence a 27-day campaign sprint for Vice President Kamala Harris next week in Pennsylvania, an adviser to the\\xa0Democratic presidential nominee’s\\xa0campaign\\xa0said, hoping his star power among Democrats can help propel her t…'}, {'title': 'ULA Vulcan launch: Live updates from Cert-2 launch countdown at Cape - Florida Today', 'content': \"Follow FLORIDA TODAY Space Team live updates from this morning's ULA Vulcan Cert-2 mission from Launch Complex 41 at Cape Canaveral Space Force Station.\"}, {'title': 'Israeli airstrikes rock southern suburbs of Beirut and cut off a key crossing into Syria - The Associated Press', 'content': 'Israel has carried out a series of massive airstrikes overnight in the southern suburbs of Beirut. Strikes have also cut off the main border crossing between Lebanon and Syria for tens of thousands of people fleeing Israeli bombardment. The blasts in Beirut’s…'}, {'title': 'Israeli airstrikes cut off a key crossing between Lebanon and Syria - NPR', 'content': 'The latest strikes came after Israel warned people to evacuate communities in southern Lebanon that are outside a United Nations-declared buffer zone.'}, {'title': 'Menendez brothers murder case under review in LA County - BBC.com', 'content': 'The brothers, who were jailed for killing their parents at a Beverly Hills mansion, say it was self-defence.'}, {'title': '[Removed]', 'content': '[Removed]'}, {'title': 'Anderson Cooper Rendered Speechless by Melania’s Book Promo - The Daily Beast', 'content': '“That’s the weirdest promo I’ve ever seen!” the CNN anchor said eventually.'}, {'title': 'Hurricane Helene now deadliest U.S. mainland storm since Katrina as death toll tops 200 - Axios', 'content': 'At least 215 deaths confirmed so far.'}, {'title': '10-year Treasury yield soars after jobs report blows past expectations - CNBC', 'content': \"In Friday's broader report, nonfarm payrolls grew by 254,000 in September, significantly exceeding economists' expectations.\"}, {'title': 'Cousins lets it fly, leads Falcons in career night - ESPN', 'content': 'Throwing confidently and decisively, Kirk Cousins passed for a career-high 509 yards and four touchdowns to lead the Falcons to an overtime win over the Bucs.'}, {'title': 'Rebel Wilson Countersues ‘The Deb’ Producers, Alleging Theft, Bullying and Sexual Misconduct - Hollywood Reporter', 'content': 'Amanda Ghost, Cameron Gregor and Vince Holden previously sued Wilson for defamation over sexual harassment and embezzlement allegations.'}, {'title': 'Trump says DOJ violated its own election rules. Lawyers disagree. - The Washington Post', 'content': 'DOJ policy advises against taking overt steps in political cases close to elections. Experts say prosecutors were following court orders in the latest Trump filing.'}, {'title': 'NFL Week 5 Recap: Immediate fantasy football takeaways from Falcons-Buccaneers Thursday Night Football - Pro Football Focus', 'content': \"PFF's fantasy football recap focuses on player usage and stats, breaking down all the vital information you need to achieve fantasy success in 2024.\"}]\n"
          ]
        }
      ],
      "source": [
        "print(articles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKW6L8__5qnh"
      },
      "source": [
        "# **DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_r7Z2dK5ngy",
        "outputId": "74f7ec7c-f377-466b-fb0f-223be96c88df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/bhavya/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/share/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m~/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/bhavya/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/share/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 11\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m     14\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())  \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/python3.12/site-packages/nltk/corpus/util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m~/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
            "File \u001b[0;32m~/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/bhavya/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/share/nltk_data'\n    - '/Users/bhavya/Desktop/NLP PROJECT/NEWS RECOMMENDATION/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())  # Tokenization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha()]  # Lemmatization\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "preprocessed_articles = [preprocess_text(article['content']) for article in articles]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik79Ldzc5qIw",
        "outputId": "e8336784-b41c-4128-a308-d9578cf6d942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['corey comperatore wa killed trump rally assassination attempt wife furious security failure led death', 'european union press ahead hefty tariff electric vehicle eu executive said friday even bloc largest economy germany rejected exposing rift biggest trade row beijing decade', 'openai ha launched new canvas interface chatgpt allows user adjust section text code generated chatbot without full rewrite', 'pretty darn confident going good day', 'removed', 'striker celebrating tentative agreement raise pay next six year', 'economy added job september sign labor market robustly healthy unemployment rate fell follow along live update stock bond market including dow jones industrial average p nasdaq', 'former president barack obama plan commence campaign sprint vice president kamala harris next week pennsylvania adviser democratic presidential nominee campaign said hoping star power among democrat help propel', 'follow florida today space team live update morning ula vulcan mission launch complex cape canaveral space force station', 'israel ha carried series massive airstrikes overnight southern suburb beirut strike also cut main border crossing lebanon syria ten thousand people fleeing israeli bombardment blast beirut', 'latest strike came israel warned people evacuate community southern lebanon outside united buffer zone', 'brother jailed killing parent beverly hill mansion say wa', 'removed', 'weirdest promo ever seen cnn anchor said eventually', 'least death confirmed far', 'friday broader report nonfarm payroll grew september significantly exceeding economist expectation', 'throwing confidently decisively kirk cousin passed yard four touchdown lead falcon overtime win bucs', 'amanda ghost cameron gregor vince holden previously sued wilson defamation sexual harassment embezzlement allegation', 'doj policy advises taking overt step political case close election expert say prosecutor following court order latest trump filing', 'pff fantasy football recap focus player usage stats breaking vital information need achieve fantasy success']\n"
          ]
        }
      ],
      "source": [
        "print(preprocessed_articles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH1Q7IbT58NN"
      },
      "source": [
        "# **Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaR4cBmm6Bq3"
      },
      "source": [
        "TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wr3BXSpF5wpP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = vectorizer.fit_transform(preprocessed_articles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG-DB2AV6D29"
      },
      "source": [
        "Word2Vec:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iLngnmTt52uo"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize the text for Word2Vec\n",
        "tokenized_articles = [article.split() for article in preprocessed_articles]\n",
        "word2vec_model = Word2Vec(tokenized_articles, vector_size=100, window=5, min_count=2, workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qESVAVZi6JG1"
      },
      "source": [
        "# **Topic Modelling using LDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mBNrcwR57D5",
        "outputId": "6d13719a-7257-435e-83b8-d409fe57be7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0:\n",
            "['expert', 'overt', 'order', 'doj', 'close', 'case', 'political', 'court', 'policy', 'advises']\n",
            "Topic 1:\n",
            "['said', 'next', 'celebrating', 'striker', 'six', 'raise', 'agreement', 'pay', 'tentative', 'year']\n",
            "Topic 2:\n",
            "['economist', 'significantly', 'exceeding', 'report', 'expectation', 'broader', 'nonfarm', 'payroll', 'campaign', 'president']\n",
            "Topic 3:\n",
            "['healthy', 'industrial', 'bond', 'stock', 'nasdaq', 'labor', 'unemployment', 'job', 'including', 'market']\n",
            "Topic 4:\n",
            "['launch', 'today', 'mission', 'ula', 'force', 'death', 'space', 'confirmed', 'far', 'least']\n",
            "Topic 5:\n",
            "['also', 'said', 'weirdest', 'cnn', 'anchor', 'eventually', 'promo', 'ever', 'seen', 'beirut']\n",
            "Topic 6:\n",
            "['economy', 'next', 'said', 'pretty', 'day', 'good', 'going', 'darn', 'confident', 'removed']\n",
            "Topic 7:\n",
            "['israel', 'zone', 'buffer', 'warned', 'evacuate', 'community', 'came', 'outside', 'united', 'fantasy']\n",
            "Topic 8:\n",
            "['corey', 'assassination', 'comperatore', 'rally', 'failure', 'attempt', 'killed', 'furious', 'led', 'wife']\n",
            "Topic 9:\n",
            "['rewrite', 'wa', 'say', 'beverly', 'hill', 'brother', 'jailed', 'killing', 'parent', 'mansion']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda_topics = lda.fit_transform(X_tfidf)\n",
        "\n",
        "# Display the topics\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Topic {idx}:\")\n",
        "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWe_KQu_60yq",
        "outputId": "6716ce1c-2940-40da-c2c9-3610a5d9189a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interaction matrix shape: (3, 4)\n"
          ]
        }
      ],
      "source": [
        "print(\"Interaction matrix shape:\", interaction_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UFSM3gNS6Mgh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example User-Article interaction matrix\n",
        "data = {'user_id': [1, 1, 2, 2, 3, 3], 'article_id': [101, 102, 101, 103, 102, 104], 'interaction': [1, 1, 1, 1, 1, 1]}\n",
        "df_interactions = pd.DataFrame(data)\n",
        "\n",
        "# Create user-item interaction matrix\n",
        "interaction_matrix = df_interactions.pivot(index='user_id', columns='article_id', values='interaction').fillna(0).values\n",
        "\n",
        "# Apply TruncatedSVD for Matrix Factorization\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "n_components = min(3, interaction_matrix.shape[1])  # Setting n_components to 3\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "collaborative_features = svd.fit_transform(interaction_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3FfE4Tga6OvW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute the cosine similarity between the articles\n",
        "content_similarity = cosine_similarity(X_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b1uIqBGC7LDF"
      },
      "outputs": [],
      "source": [
        "def hybrid_recommendation(user_id, user_articles, collaborative_features, content_similarity):\n",
        "    # Collaborative Filtering: Based on user similarity\n",
        "    collaborative_recommendations = collaborative_features[user_id]\n",
        "\n",
        "    # Content-Based Filtering: Based on article similarity\n",
        "    content_recommendations = np.mean([content_similarity[article] for article in user_articles], axis=0)\n",
        "\n",
        "    # Fusion Layer: Combine both collaborative and content-based filtering\n",
        "    final_recommendations = (collaborative_recommendations + content_recommendations) / 2\n",
        "    return np.argsort(final_recommendations)[::-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6g4BsSs7NMy",
        "outputId": "5ee93f67-3fbb-4fe0-b2e4-ad5bb995a75d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.8333333333333333, Recall: 0.75\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "def evaluate_recommendation(true_labels, predicted_labels):\n",
        "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "    return precision, recall\n",
        "\n",
        "# Example usage:\n",
        "true_labels = [1, 0, 1, 0]  # True interaction labels\n",
        "predicted_labels = [1, 1, 1, 0]  # Predicted labels from recommendation system\n",
        "\n",
        "precision, recall = evaluate_recommendation(true_labels, predicted_labels)\n",
        "print(f\"Precision: {precision}, Recall: {recall}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHSoisH97PW-",
        "outputId": "f3c07c5f-c52d-472c-c116-aa12f7033127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/recommend', methods=['POST'])\n",
        "def recommend():\n",
        "    user_data = request.json\n",
        "    user_id = user_data['user_id']\n",
        "    user_articles = user_data['articles']\n",
        "\n",
        "    recommendations = hybrid_recommendation(user_id, user_articles, collaborative_features, content_similarity)\n",
        "    return jsonify({'recommendations': recommendations.tolist()})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BKpXG3e7hAV"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.title(\"Personalized News Recommendation System\")\n",
        "\n",
        "user_input = st.text_input(\"Enter your user ID:\")\n",
        "if st.button('Get Recommendations'):\n",
        "    user_id = int(user_input)\n",
        "    user_articles = [101, 102]  # Example: Articles the user has interacted with\n",
        "\n",
        "    recommendations = hybrid_recommendation(user_id, user_articles, collaborative_features, content_similarity)\n",
        "    st.write(f\"Recommended Articles: {recommendations[:5]}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
